{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# se instala apache airflow "
      ],
      "metadata": {
        "id": "ku2kyATE3mA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install apache-airflow\n",
        "!pip install apache-airflow-upgrade-check"
      ],
      "metadata": {
        "id": "xW-g-QzlBycn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'apache-airflow[postgres,google]==2.5.0' \\--constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.5.0/constraints-3.7.txt\""
      ],
      "metadata": {
        "id": "0uzzNyYXh5No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# se importan las bibliotecas principales principales"
      ],
      "metadata": {
        "id": "jFbP7r2JuuOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# funcion etl que integra los tres procesos de transformacion"
      ],
      "metadata": {
        "id": "EbOAy34puUeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install tweepy\n",
        "!pip install --upgrade tweepy\n",
        "!pip show tweepy\n",
        "import tweepy\n",
        "from tweepy import API\n",
        "import re\n",
        "from tweepy import Stream\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from random import randint\n",
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.metrics import *\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def etl():\n",
        "  \"\"\"Collect  tweets matching the given query related with energy markets. with min likes and min retweets\n",
        "  \n",
        "\n",
        "   \n",
        "    query (str): The search query to use with specific words  like invest or up down electricity \n",
        "    max_tweets (int): The maximum number of tweets to collect.\n",
        "    min_retweets just to filter tweets with good amount of retwwets  which is set to 1000\n",
        "  \n",
        "  \"\"\"\n",
        "  import config\n",
        "  consumer_key = config.consumer_key\n",
        "  consumer_secret = config.consumer_secret\n",
        "  access_token = config.access_token\n",
        "  access_token_secret = config.access_token_secret\n",
        "  \n",
        "\n",
        "  words=[\"positive\",\"Invest\", \"up\",\"down\",\"Price up\",\"win\",\"stock\",\"fired\"]\n",
        "\n",
        "  query=[\"Invest\"or\"up\"or\"Energy\"or\"Price up\"or\"win\"or\"stock\"or\"Natural Gas\"or\"coal\"]\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  min_likes=0\n",
        "  min_retweets=1000\n",
        "  auth = tweepy.OAuth1UserHandler(\n",
        "      consumer_key, consumer_secret, access_token, access_token_secret\n",
        "  )\n",
        "\n",
        "  api = tweepy.API(auth)\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "# search for tweets since the time 15 minutes ago\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "#extract and log \n",
        "  logging.info(\"extrayendo \")\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "  #create df\n",
        "  df = pd.DataFrame(columns=['created_at', 'text', 'screen_name', 'retweets', 'favorites','negative sentiment', 'positive sentiment', \"#words\"])\n",
        "  # create a sentiment analyzer\n",
        "  sid = SentimentIntensityAnalyzer()\n",
        "  \n",
        "  # create a dictionary to store the results\n",
        "  \n",
        "  processed_tweets = []\n",
        "  contador_Palabras=0\n",
        "  logging.info(\"transformando\")\n",
        "  tweets = tweepy.Cursor(api.search_tweets,tweet_mode='extended', q=query, lang='en')\n",
        "  for tweet in tweets:\n",
        "    #tweets.append(tweet.text)\n",
        "    if tweet.favorite_count >= min_likes and tweet.retweet_count >= min_retweets:\n",
        "      #print(tweet.text)\n",
        "      # en esta etapa se tokeniza el tweet para y poder limpiarlo por tipo de palabra\n",
        "      words = nltk.word_tokenize(re.sub(r'https?://\\S+', '', tweet.text))\n",
        "      #words = nltk.word_tokenize( tweet.text)\n",
        "      #  remove the punctuation\n",
        "      words = [word for word in words if word.isalpha()]\n",
        "      \n",
        "      # words to lowercase\n",
        "      words = [word.lower() for word in words]\n",
        "\n",
        "      #  remove the stop words from the tweet\n",
        "      stop_words = nltk.corpus.stopwords.words('english')\n",
        "      words = [word for word in words if word not in stop_words]\n",
        "      contador_Palabras=len(words)\n",
        "      # Stemming en esta etapa se llevan las palabras a su raiz \n",
        "      stemmer = nltk.SnowballStemmer('english')\n",
        "      words = [stemmer.stem(word) for word in words]\n",
        "      processed_tweets.append(words)\n",
        "      #print(type())\n",
        "      sentiment1 = sid.polarity_scores(tweet.text)\n",
        "      scores = []\n",
        "\n",
        "      scores.append(sid.polarity_scores(tweet.text)['compound'])\n",
        "\n",
        "            \n",
        "            \n",
        "      df = df.append({\n",
        "                      'created_at': tweet.created_at,\n",
        "                      'text': tweet.text,\n",
        "                      'screen_name': tweet.user.screen_name, \n",
        "                      'retweets': tweet.retweet_count,\n",
        "                      'favorites': tweet.favorite_count,\n",
        "                      'negative sentiment':min(scores),\n",
        "                      'positive sentiment':max(scores),\n",
        "                      \"#words\":contador_Palabras,\n",
        "\n",
        "                      }, ignore_index=True)\n",
        "      \n",
        "      \n",
        "          \n",
        "  # en esta etapa se carga el df como csv para su posterior visulaizacion en power BI\n",
        "  logging.info(\"cargando\")\n",
        "  df.to_csv(\"data1.csv\", index=False)\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsCGzUBHkQXT",
        "outputId": "dd723ff4-1767-4cbc-9c55-1cd6060df7a6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.8/dist-packages (4.12.1)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.8/dist-packages (from tweepy) (2.28.1)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.27.0->tweepy) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.27.0->tweepy) (1.26.13)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.27.0->tweepy) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.8/dist-packages (4.12.1)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.8/dist-packages (from tweepy) (2.28.1)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.27.0->tweepy) (1.26.13)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.27.0->tweepy) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.27.0->tweepy) (2022.9.24)\n",
            "Name: tweepy\n",
            "Version: 4.12.1\n",
            "Summary: Twitter library for Python\n",
            "Home-page: https://www.tweepy.org/\n",
            "Author: Joshua Roesslein\n",
            "Author-email: tweepy@googlegroups.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.8/dist-packages\n",
            "Requires: oauthlib, requests, requests-oauthlib\n",
            "Required-by: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DAG en esta celda se delclaran los atributos del dag y el task que debe realizar \n"
      ],
      "metadata": {
        "id": "HqfieZdsucyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.utils.dates import days_ago\n",
        "import psycopg2\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "default_args={\n",
        "    'owner': 'enersinc, carlos saenz',\n",
        "    'email':['carlosaenz.26@hotmail.com'],\n",
        "    'depends_on_past':False,\n",
        "    'email_on_failure':['carlosaenz.26@hotmail.com'],\n",
        "    'retries':3,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "} \n",
        "\n",
        "dag=DAG(\n",
        "    'twitter_ETL',\n",
        "    default_args=default_args,\n",
        "    description='tweets investmen ETL enersinc prueba Carlos S',\n",
        "    schedule_interval=timedelta(days=3),\n",
        "    start_date=days_ago(2),\n",
        "\n",
        ")\n",
        "\n",
        "twiter_etl=PythonOperator(task_id=\"complete_etl\",\n",
        "                              python_callable=etl\n",
        "                            ,dag=dag)\n",
        "\n",
        "\n",
        "\n",
        "twiter_etl\n"
      ],
      "metadata": {
        "id": "ORqRlHA9zyhk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "336e5c79-dd20-4012-c252-33d3c0889bf1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m<\u001b[0m\u001b[1;33mipython-input-\u001b[0m\u001b[1;33m51\u001b[0m\u001b[1;33m-7c7f9ced12db\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m22\u001b[0m\u001b[1;33m RemovedInAirflow3Warning\u001b[0m\u001b[33m: Function `days_ago` is deprecated and will be removed in Airflow \u001b[0m\u001b[1;33m3.0\u001b[0m\u001b[33m. You can achieve equivalent behavior with `\u001b[0m\u001b[1;33mpendulum.today\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33m'UTC'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mdays\u001b[0m\u001b[33m=-N, \u001b[0m\u001b[33m...\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m`\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">&lt;ipython-input-</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">51</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">-7c7f9ced12db&gt;:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">22</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> RemovedInAirflow3Warning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Function `days_ago` is deprecated and will be removed in Airflow </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3.0</span><span style=\"color: #808000; text-decoration-color: #808000\">. You can achieve equivalent behavior with `</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pendulum.today(</span><span style=\"color: #808000; text-decoration-color: #808000\">'UTC'</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">).add(</span><span style=\"color: #808000; text-decoration-color: #808000\">days</span><span style=\"color: #808000; text-decoration-color: #808000\">=-N, ...</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">`</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m<\u001b[0m\u001b[1;33mipython-input-\u001b[0m\u001b[1;33m51\u001b[0m\u001b[1;33m-7c7f9ced12db\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m17\u001b[0m\u001b[1;33m RemovedInAirflow3Warning\u001b[0m\u001b[33m: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">&lt;ipython-input-</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">51</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">-7c7f9ced12db&gt;:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">17</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> RemovedInAirflow3Warning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Task(PythonOperator): complete_etl>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0fAgaeti2Ay8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "          ┌──────────┐\n",
        "          │ Autenticación │\n",
        "          └──────────┘\n",
        "                 ▲\n",
        "                 └──────┐\n",
        "          ┌──────────┐  │\n",
        "          │ Extracción de Tweets │\n",
        "          └──────────┘  │\n",
        "                 ▲      └──────┐\n",
        "                 └──────┐  ┌─────────────┐\n",
        "      ┌──────────────┐  │  │Extracción de│ \n",
        "      │              │     │tweets por palabras\n",
        "                               claves    │ \n",
        "      │              │     │             │\n",
        "      Transformacion de │  └─────────────┘\n",
        "      │ tweets   │\n",
        "      └─────────────┘  │  \n",
        "                 ▲      └──────┐\n",
        "                 └──────┐  ┌──────────────────┐\n",
        "          ┌──────────┐  │  │ Almacenamiento   │\n",
        "          └──────────┘     └──────────────────┘\n"
      ],
      "metadata": {
        "id": "PmKAvSEKDWRu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s6ak0FaFGFlO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}